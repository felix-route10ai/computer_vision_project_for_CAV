{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1abOFXUak_XnpOlOs4nEPEpfPyGX4T-Rw",
      "authorship_tag": "ABX9TyPPb+z5U962QR/Jy69A97Xq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felix-route10ai/computer_vision_project_for_CAV/blob/main/CAV_OSM_Feature_Extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q googlemaps polyline pandas"
      ],
      "metadata": {
        "id": "DpYYl5XVclJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import math\n",
        "import polyline\n",
        "import pandas as pd\n",
        "import googlemaps\n",
        "\n",
        "# Google API key\n",
        "API_KEY = \"<your google maps API key>\"\n",
        "gmaps = googlemaps.Client(key=API_KEY)\n",
        "\n",
        "# === 1. Define origin/destination ===\n",
        "origin = \"Port of Dover, Dover CT16 1JA\"\n",
        "destination = \"Magna Park, Plot 510, GLP, Milton Keynes MK17 7AB\"\n",
        "\n",
        "# === 2. Fetch directions from Google Maps ===\n",
        "directions = gmaps.directions(origin, destination, mode=\"driving\", units=\"metric\")\n",
        "steps = directions[0][\"legs\"][0][\"steps\"]\n",
        "\n",
        "# === 3. Decode polylines from each step ===\n",
        "segments = []\n",
        "for i, step in enumerate(steps):\n",
        "    points = polyline.decode(step[\"polyline\"][\"points\"])\n",
        "    for lat, lng in points:\n",
        "        segments.append({\n",
        "            \"segment_id\": i + 1,\n",
        "            \"instruction\": step.get(\"html_instructions\", \"\"),\n",
        "            \"lat\": lat,\n",
        "            \"lng\": lng\n",
        "        })\n",
        "\n",
        "df_route = pd.DataFrame(segments)\n",
        "\n",
        "# === 4. Save full route to Google Drive ===\n",
        "output_dir = \"<your output directory>\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "df_route.to_csv(f\"{output_dir}/route_segments.csv\", index=False)\n",
        "print(f\"route_segments.csv saved with {len(df_route)} rows\")\n",
        "\n",
        "# === Helper: Haversine Distance (in km) ===\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in km\n",
        "    dlat = math.radians(lat2 - lat1)\n",
        "    dlon = math.radians(lon2 - lon1)\n",
        "    a = math.sin(dlat / 2) ** 2 + math.cos(math.radians(lat1)) * \\\n",
        "        math.cos(math.radians(lat2)) * math.sin(dlon / 2) ** 2\n",
        "    return R * 2 * math.asin(math.sqrt(a))\n",
        "\n",
        "# === Resample the route every X meters ===\n",
        "def resample_route(df_route, spacing_m=50):\n",
        "    resampled = [df_route.iloc[0]]\n",
        "    accum_distance = 0\n",
        "    last_lat, last_lng = df_route.iloc[0][\"lat\"], df_route.iloc[0][\"lng\"]\n",
        "\n",
        "    for _, row in df_route.iterrows():\n",
        "        dist = haversine(last_lat, last_lng, row[\"lat\"], row[\"lng\"]) * 1000  # km to meters\n",
        "        accum_distance += dist\n",
        "        if accum_distance >= spacing_m:\n",
        "            resampled.append(row)\n",
        "            last_lat, last_lng = row[\"lat\"], row[\"lng\"]\n",
        "            accum_distance = 0\n",
        "\n",
        "    df_resampled = pd.DataFrame(resampled).reset_index(drop=True)\n",
        "    df_resampled[\"segment_id\"] = df_resampled.index + 1\n",
        "    return df_resampled\n",
        "\n",
        "# === Call it with 50m spacing ===\n",
        "df_resampled_50m = resample_route(df_route, spacing_m=50)\n",
        "df_resampled_50m.to_csv(\"<your output directory>/df_resampled_50m.csv\", index=False)\n",
        "print(\"Saved df_resampled_50m.csv with\", len(df_resampled_50m), \"segments.\")\n",
        "df_resampled_50m.head()"
      ],
      "metadata": {
        "id": "iMYHu-p1c2Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googlemaps polyline geopandas shapely"
      ],
      "metadata": {
        "id": "q1wYR2HmgdpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idgz4afDXMy1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# CAV Risk POI Extractor (Google Maps Focused)\n",
        "\n",
        "# This notebook will:\n",
        "# 1. Load route segments from a CSV (lat/lng)\n",
        "# 2. Use Google Maps Directions API to get route polyline\n",
        "# 3. Use Google Roads API to snap points and enrich with road geometry\n",
        "# 4. Apply rules to detect features (lane merges, curves, etc.)\n",
        "# 5. Save POIs with lat/lng and type to Drive for image download\n",
        "\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import googlemaps\n",
        "import polyline\n",
        "import time\n",
        "from shapely.geometry import LineString, Point\n",
        "import os\n",
        "\n",
        "# === CONFIG ===\n",
        "API_KEY = \"<your google maps API key>\"\n",
        "ROUTE_CSV = \"<your output directory>/df_resampled_50m.csv\"\n",
        "OUTPUT_POIS_CSV = \"<your output directory>/route_pois.csv\"\n",
        "MAX_WAYPOINTS_PER_REQUEST = 23  # Google API limit is 25 (start + end + 23)\n",
        "REQUEST_DELAY_SEC = 2\n",
        "\n",
        "# === INIT ===\n",
        "gmaps = googlemaps.Client(key=API_KEY)\n",
        "\n",
        "# === 1. Load Route ===\n",
        "df_route = pd.read_csv(ROUTE_CSV)\n",
        "waypoints = df_route[[\"lat\", \"lng\"]].values.tolist()\n",
        "\n",
        "# === 2. Chunked API Calls ===\n",
        "def fetch_directions_in_chunks(points):\n",
        "    all_snapped = []\n",
        "    for i in range(0, len(points) - 1, MAX_WAYPOINTS_PER_REQUEST):\n",
        "        chunk_start = points[i]\n",
        "        chunk_end = points[min(i + MAX_WAYPOINTS_PER_REQUEST, len(points) - 1)]\n",
        "        chunk_via = points[i + 1:min(i + MAX_WAYPOINTS_PER_REQUEST, len(points) - 1)]\n",
        "\n",
        "        print(f\"Fetching segment {i} to {i + MAX_WAYPOINTS_PER_REQUEST}...\")\n",
        "        try:\n",
        "            directions = gmaps.directions(origin=chunk_start, destination=chunk_end, waypoints=chunk_via, mode=\"driving\")\n",
        "            polyline_str = directions[0][\"overview_polyline\"][\"points\"]\n",
        "            snapped_points = polyline.decode(polyline_str)\n",
        "            all_snapped.extend(snapped_points)\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching directions for segment {i}: {e}\")\n",
        "\n",
        "        time.sleep(REQUEST_DELAY_SEC)\n",
        "\n",
        "    return all_snapped\n",
        "\n",
        "# === 3. Get full snapped route ===\n",
        "snapped_points = fetch_directions_in_chunks(waypoints)\n",
        "\n",
        "# === 4. Feature Detection Rules ===\n",
        "def calculate_bearing(p1, p2):\n",
        "    import math\n",
        "    lat1, lon1, lat2, lon2 = map(math.radians, [p1[0], p1[1], p2[0], p2[1]])\n",
        "    dlon = lon2 - lon1\n",
        "    y = math.sin(dlon) * math.cos(lat2)\n",
        "    x = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(dlon)\n",
        "    bearing = math.atan2(y, x)\n",
        "    return (math.degrees(bearing) + 360) % 360\n",
        "\n",
        "def distance(p1, p2):\n",
        "    from geopy.distance import geodesic\n",
        "    return geodesic(p1, p2).meters\n",
        "\n",
        "poi_records = []\n",
        "for i in range(2, len(snapped_points) - 2):\n",
        "    prev = snapped_points[i - 1]\n",
        "    curr = snapped_points[i]\n",
        "    next_ = snapped_points[i + 1]\n",
        "\n",
        "    # === CURVE OR JUNCTION ===\n",
        "    angle = abs(calculate_bearing(prev, curr) - calculate_bearing(curr, next_))\n",
        "    if angle > 45:\n",
        "        poi_records.append({\"lat\": curr[0], \"lng\": curr[1], \"feature\": \"curve_or_junction\"})\n",
        "\n",
        "    # === LANE MERGE HEURISTIC ===\n",
        "    d1 = distance(prev, curr)\n",
        "    d2 = distance(curr, next_)\n",
        "    d_change = abs(d2 - d1)\n",
        "    if d_change > 10 and angle < 20:\n",
        "        poi_records.append({\"lat\": curr[0], \"lng\": curr[1], \"feature\": \"possible_lane_merge\"})\n",
        "\n",
        "# === 5. Save POIs ===\n",
        "df_pois = pd.DataFrame(poi_records)\n",
        "df_pois.to_csv(OUTPUT_POIS_CSV, index=False)\n",
        "\n",
        "print(f\"Saved {len(df_pois)} POIs to: {OUTPUT_POIS_CSV}\")\n",
        "df_pois.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pillow"
      ],
      "metadata": {
        "id": "KyMICSo-irrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POI Image Downloader\n",
        "\n",
        "!pip install pillow requests\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import math\n",
        "import time\n",
        "\n",
        "# === CONFIG ===\n",
        "API_KEY = \"<your google maps API key>\"\n",
        "POIS_CSV = \"<your output directory>/route_pois.csv\"\n",
        "OUTPUT_DIR = \"<your output directory>/poi_images\"\n",
        "\n",
        "# === HELPER FUNCTIONS ===\n",
        "def calculate_heading(lat1, lng1, lat2, lng2):\n",
        "    lat1, lng1, lat2, lng2 = map(math.radians, [lat1, lng1, lat2, lng2])\n",
        "    dlon = lng2 - lng1\n",
        "    y = math.sin(dlon) * math.cos(lat2)\n",
        "    x = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(dlon)\n",
        "    bearing = math.degrees(math.atan2(y, x))\n",
        "    return (bearing + 360) % 360\n",
        "\n",
        "def optimized_download(df, api_key, output_dir, start_index=0, max_images=None):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    base_url = \"https://maps.googleapis.com/maps/api/streetview\"\n",
        "\n",
        "    DELAY_BETWEEN_REQUESTS = 2\n",
        "    DELAY_AFTER_ERROR = 15\n",
        "    MAX_RETRIES = 3\n",
        "\n",
        "    image_records = []\n",
        "    error_log = []\n",
        "\n",
        "    end_index = len(df) if not max_images else min(start_index + max_images, len(df))\n",
        "\n",
        "    print(f\"Optimized download starting...\")\n",
        "    print(f\"Processing rows {start_index} to {end_index-1} ({end_index-start_index} images)\")\n",
        "    print(f\"This will take ~{(end_index-start_index) * DELAY_BETWEEN_REQUESTS / 60:.1f} minutes\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i in range(start_index, end_index):\n",
        "        row = df.iloc[i]\n",
        "        lat = row[\"lat\"]\n",
        "        lng = row[\"lng\"]\n",
        "        feature = row[\"feature\"]\n",
        "        segment_id = i + 1  # or use a unique ID field\n",
        "\n",
        "        location = f\"{lat},{lng}\"\n",
        "        image_path = f\"{output_dir}/poi_{segment_id:04d}_{feature}.jpg\"\n",
        "\n",
        "        heading = 0\n",
        "        if i < len(df) - 1:\n",
        "            next_row = df.iloc[i + 1]\n",
        "            heading = calculate_heading(lat, lng, next_row[\"lat\"], next_row[\"lng\"])\n",
        "\n",
        "        if os.path.exists(image_path):\n",
        "            print(f\"[{i}] Exists - Skipping\")\n",
        "            image_records.append({\"poi_id\": segment_id, \"lat\": lat, \"lng\": lng, \"feature\": feature, \"image_path\": image_path})\n",
        "            continue\n",
        "\n",
        "        params = {\n",
        "            \"size\": \"640x640\",\n",
        "            \"location\": location,\n",
        "            \"fov\": 90,\n",
        "            \"pitch\": 0,\n",
        "            \"heading\": heading,\n",
        "            \"key\": api_key\n",
        "        }\n",
        "\n",
        "        success = False\n",
        "        for attempt in range(MAX_RETRIES):\n",
        "            try:\n",
        "                response = requests.get(base_url, params=params, timeout=30)\n",
        "                if response.status_code == 200 and 'image' in response.headers.get('content-type', ''):\n",
        "                    img = Image.open(BytesIO(response.content))\n",
        "                    img.save(image_path)\n",
        "                    image_records.append({\"poi_id\": segment_id, \"lat\": lat, \"lng\": lng, \"feature\": feature, \"image_path\": image_path})\n",
        "                    print(f\"[{i}] Saved {os.path.basename(image_path)}\")\n",
        "                    success = True\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"[{i}] Invalid content or status code: {response.status_code}\")\n",
        "                    time.sleep(DELAY_AFTER_ERROR)\n",
        "            except Exception as e:\n",
        "                print(f\"[{i}] Exception: {e}\")\n",
        "                time.sleep(DELAY_AFTER_ERROR)\n",
        "\n",
        "        if not success:\n",
        "            error_log.append({\"poi_id\": segment_id, \"lat\": lat, \"lng\": lng, \"feature\": feature, \"error\": \"Failed after retries\"})\n",
        "\n",
        "        if i < end_index - 1:\n",
        "            time.sleep(DELAY_BETWEEN_REQUESTS)\n",
        "\n",
        "    df_images = pd.DataFrame(image_records)\n",
        "    df_images.to_csv(f\"{output_dir}/streetview_metadata.csv\", index=False)\n",
        "    if error_log:\n",
        "        pd.DataFrame(error_log).to_csv(f\"{output_dir}/error_log.csv\", index=False)\n",
        "\n",
        "    print(f\"Downloaded {len(image_records)} images, {len(error_log)} errors logged.\")\n",
        "    return df_images, error_log\n",
        "\n",
        "# === USAGE ===\n",
        "df = pd.read_csv(POIS_CSV)\n",
        "images, errors = optimized_download(df, API_KEY, OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "WZkI-Ng8iwXl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a traning data set for manual annotation\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# Load metadata\n",
        "metadata_path = \"<your output directory>/streetview_metadata.csv\"\n",
        "df = pd.read_csv(metadata_path)\n",
        "\n",
        "# Make sure paths are strings (in case they're float NaNs)\n",
        "df[\"image_path\"] = df[\"image_path\"].astype(str)\n",
        "\n",
        "# Target output folder\n",
        "output_dir = \"<your output directory>/annotation_sample_images\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Sample 200 images (stratified by feature, capped if needed)\n",
        "sampled_dfs = []\n",
        "samples_per_class = max(1, 200 // df[\"feature\"].nunique())\n",
        "\n",
        "for feature, group in df.groupby(\"feature\"):\n",
        "    sampled = group.sample(n=min(len(group), samples_per_class), random_state=42)\n",
        "    sampled_dfs.append(sampled)\n",
        "\n",
        "df_sampled = pd.concat(sampled_dfs).reset_index(drop=True)\n",
        "\n",
        "# Copy selected images to annotation folder\n",
        "copied = 0\n",
        "for _, row in df_sampled.iterrows():\n",
        "    src = row[\"image_path\"]\n",
        "    if os.path.exists(src):\n",
        "        fname = os.path.basename(src)\n",
        "        dst = os.path.join(output_dir, fname)\n",
        "        shutil.copy(src, dst)\n",
        "        copied += 1\n",
        "\n",
        "print(f\"Sampled {len(df_sampled)} images\")\n",
        "print(f\"Copied {copied} images to: {output_dir}\")"
      ],
      "metadata": {
        "id": "ao7zdlWLDto9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "yaI1ZafJ4hfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "model.train(data=\"<your output directory>/data.yaml\", epochs=50, imgsz=640)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OJUhzJZ04OBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run Inference on using Traning Data and Full Dataset\n",
        "\n",
        "#Imports and Paths\n",
        "import os\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "#Paths\n",
        "MODEL_PATH = \"<your model path>/best.pt\"\n",
        "IMAGE_DIR = \"<your image directory/poi_images\"\n",
        "OUTPUT_DIR = \"<your output directory>/inference_results\"\n",
        "CSV_OUTPUT = f\"{OUTPUT_DIR}/detection_summary.csv\"\n",
        "\n",
        "# Ensure output directory is clean\n",
        "if os.path.exists(OUTPUT_DIR):\n",
        "    shutil.rmtree(OUTPUT_DIR)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "#Load model\n",
        "model = YOLO(MODEL_PATH)\n",
        "\n",
        "#Run inference\n",
        "image_files = [os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR) if f.endswith(('.jpg', '.png'))]\n",
        "\n",
        "print(f\"Running inference on {len(image_files)} images...\")\n",
        "\n",
        "results_summary = []\n",
        "\n",
        "for img_path in tqdm(image_files):\n",
        "    result = model.predict(img_path, conf=0.4, save=True, save_txt=False, save_crop=False, project=OUTPUT_DIR, name=\"pred\", exist_ok=True)\n",
        "    dets = result[0].boxes\n",
        "    if dets is not None and dets.cls.numel() > 0:\n",
        "        for cls, conf in zip(dets.cls.cpu().numpy(), dets.conf.cpu().numpy()):\n",
        "            results_summary.append({\n",
        "                \"image_file\": os.path.basename(img_path),\n",
        "                \"class_id\": int(cls),\n",
        "                \"confidence\": float(conf)\n",
        "            })\n",
        "    else:\n",
        "        results_summary.append({\n",
        "            \"image_file\": os.path.basename(img_path),\n",
        "            \"class_id\": None,\n",
        "            \"confidence\": None\n",
        "        })\n",
        "\n",
        "#Save CSV Summary\n",
        "df_results = pd.DataFrame(results_summary)\n",
        "df_results.to_csv(CSV_OUTPUT, index=False)\n",
        "print(f\"\\n Inference complete! Results saved to: {CSV_OUTPUT}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qAgKPJoV8IP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inference metadataset\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "\n",
        "#CONFIG\n",
        "PRED_IMAGE_DIR = \"<your directory>/predict\"\n",
        "ORIGINAL_METADATA_CSV = \"<your directory>/streetview_metadata.csv\"\n",
        "LABELS_DIR = os.path.join(PRED_IMAGE_DIR, \"labels\")  # YOLO stores txt labels here\n",
        "ANNOTATED_IMAGES_DIR = PRED_IMAGE_DIR               # Annotated image outputs\n",
        "OUTPUT_CSV = \"<your directory>/inference_metadata.csv\"\n",
        "\n",
        "#Load original metadata\n",
        "df_meta = pd.read_csv(ORIGINAL_METADATA_CSV)\n",
        "df_meta[\"filename\"] = df_meta[\"image_path\"].apply(lambda x: os.path.basename(x))\n",
        "\n",
        "#Load class map from YOLO model (use your trained model)\n",
        "model = YOLO(\"<your model path>/best.pt\")  # adjust if needed\n",
        "class_map = model.names  # e.g. {0: 'roundabout', 1: 'tunnel', ...}\n",
        "\n",
        "#Collect predictions\n",
        "records = []\n",
        "for i, row in df_meta.iterrows():\n",
        "    filename = row[\"filename\"]\n",
        "    label_file = os.path.join(LABELS_DIR, filename.replace(\".jpg\", \".txt\"))\n",
        "    annotated_path = os.path.join(ANNOTATED_IMAGES_DIR, filename)\n",
        "\n",
        "    if not os.path.exists(label_file):\n",
        "        continue\n",
        "\n",
        "    with open(label_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 5:\n",
        "                continue  # Skip invalid lines\n",
        "\n",
        "            class_id = int(float(parts[0]))\n",
        "            conf = float(parts[5]) if len(parts) > 5 else None\n",
        "\n",
        "            records.append({\n",
        "                \"lat\": row[\"lat\"],\n",
        "                \"lng\": row[\"lng\"],\n",
        "                \"feature\": class_map[class_id],\n",
        "                \"confidence\": conf,\n",
        "                \"image_path\": annotated_path\n",
        "            })\n",
        "\n",
        "# === Save metadata\n",
        "df_output = pd.DataFrame(records)\n",
        "df_output.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"Inference metadata saved: {OUTPUT_CSV}\")\n",
        "df_output.head()"
      ],
      "metadata": {
        "id": "MK55c-46OnQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Join data files for use with a Map - Option Streamlit used in this project\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "#Paths\n",
        "DETECTION_CSV = \"<your storage directory>/detection_summary.csv\"\n",
        "ORIGINAL_META = \"<your image directory>/streetview_metadata.csv\"\n",
        "YOLO_MODEL_PATH = \"<your model directory>/best.pt\"\n",
        "OUTPUT_CSV = \"<directory>/streamlit_metadata.csv\"\n",
        "\n",
        "#Load data\n",
        "df_det = pd.read_csv(DETECTION_CSV)\n",
        "df_meta = pd.read_csv(ORIGINAL_META)\n",
        "df_meta[\"image_file\"] = df_meta[\"image_path\"].apply(lambda x: os.path.basename(x))\n",
        "\n",
        "#Drop rows with missing class_id\n",
        "df_det_clean = df_det.dropna(subset=[\"class_id\"]).copy()\n",
        "\n",
        "#Load YOLO class map\n",
        "model = YOLO(YOLO_MODEL_PATH)\n",
        "class_map = model.names  # e.g. {0: 'roundabout', 1: 'lane_merge', ...}\n",
        "\n",
        "#Map class_id to labels\n",
        "df_det_clean[\"class_id\"] = df_det_clean[\"class_id\"].astype(int)\n",
        "df_det_clean[\"feature\"] = df_det_clean[\"class_id\"].map(class_map)\n",
        "\n",
        "#Merge with original lat/lng + image_path\n",
        "df_joined = df_det_clean.merge(df_meta[[\"image_file\", \"lat\", \"lng\", \"image_path\"]], on=\"image_file\", how=\"left\")\n",
        "\n",
        "#Reorder and save\n",
        "df_final = df_joined[[\"lat\", \"lng\", \"feature\", \"confidence\", \"image_path\"]]\n",
        "df_final.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "print(f\"Cleaned Streamlit metadata saved to: {OUTPUT_CSV}\")\n",
        "df_final.head()"
      ],
      "metadata": {
        "id": "h1ebSRdhQdo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Streamlit Map Visualisation for CAV Risk Detection\n",
        "\n",
        "# Install dependencies\n",
        "!pip install streamlit folium pandas geopandas -q\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "from streamlit_folium import st_folium\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "#CONFIG\n",
        "ROUTE_CSV = \"<your directory>/df_resampled_50m.csv\"\n",
        "INFERENCE_CSV = \"<your directory>/inference_results.csv\"  # Must contain: lat, lng, class_name, image_path\n",
        "\n",
        "#Evalution criteria - probably needs to be changed\n",
        "CRITICAL = [\"tunnel\", \"roundabout\", \"lane_merge\", \"construction_zone\"]\n",
        "MEDIUM = [\"junction\", \"curve\", \"pedestrian_crossing\", \"obscured_signage\", \"multiple_lanes\"]\n",
        "LOW = [\"bus_stop\", \"signage\", \"no_hard_shoulder\", \"residential\", \"urban\", \"parked_cars\", \"foggy\"]\n",
        "\n",
        "RISK_COLOR = {\n",
        "    \"CRITICAL\": \"red\",\n",
        "    \"MEDIUM\": \"orange\",\n",
        "    \"LOW\": \"green\"\n",
        "}\n",
        "\n",
        "# === LOAD DATA ===\n",
        "df_route = pd.read_csv(ROUTE_CSV)\n",
        "df_infer = pd.read_csv(INFERENCE_CSV)\n",
        "\n",
        "# === CLASSIFY RISK ===\n",
        "def get_risk_level(cls):\n",
        "    if cls in CRITICAL:\n",
        "        return \"CRITICAL\"\n",
        "    elif cls in MEDIUM:\n",
        "        return \"MEDIUM\"\n",
        "    else:\n",
        "        return \"LOW\"\n",
        "\n",
        "df_infer[\"risk\"] = df_infer[\"class_name\"].apply(get_risk_level)\n",
        "\n",
        "# === STREAMLIT UI ===\n",
        "st.set_page_config(layout=\"wide\")\n",
        "st.title(\"CAV Route Risk Visualisation\")\n",
        "\n",
        "features = st.multiselect(\"Select features to display:\", options=sorted(df_infer[\"class_name\"].unique()), default=None)\n",
        "\n",
        "# === CREATE MAP ===\n",
        "center_lat = df_route[\"lat\"].mean()\n",
        "center_lng = df_route[\"lng\"].mean()\n",
        "map_ = folium.Map(location=[center_lat, center_lng], zoom_start=9)\n",
        "\n",
        "# Draw route in black\n",
        "coords = df_route[[\"lat\", \"lng\"]].values.tolist()\n",
        "folium.PolyLine(coords, color=\"black\", weight=3).add_to(map_)\n",
        "\n",
        "# Filter by selected features\n",
        "if features:\n",
        "    df_show = df_infer[df_infer[\"class_name\"].isin(features)]\n",
        "else:\n",
        "    df_show = df_infer\n",
        "\n",
        "# Add POI markers\n",
        "cluster = MarkerCluster().add_to(map_)\n",
        "for _, row in df_show.iterrows():\n",
        "    popup_html = f\"\"\"\n",
        "    <b>Class:</b> {row['class_name']}<br>\n",
        "    <b>Risk:</b> {row['risk']}<br>\n",
        "    <img src=\"file://{row['image_path']}\" width=\"300\">\n",
        "    \"\"\"\n",
        "    folium.Marker(\n",
        "        location=[row[\"lat\"], row[\"lng\"]],\n",
        "        popup=folium.Popup(popup_html, max_width=300),\n",
        "        icon=folium.Icon(color=RISK_COLOR[row[\"risk\"]])\n",
        "    ).add_to(cluster)\n",
        "\n",
        "# Render map\n",
        "st_data = st_folium(map_, width=1400, height=700)"
      ],
      "metadata": {
        "id": "yFWmpfEYOJVI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}